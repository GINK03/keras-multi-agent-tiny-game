# Multi Agent Deep Learning for Keras

## Kerasでマルチエージェントラーニング
マルチエージェントラーニングは、相互に影響を与え合うモデルが強調ないし、敵対して、目的となる報酬を最大化するシチュエーションのディープラーニングです[1][2]

強化学習の特殊系と捉えることができそです　　

Deep Mind社が提案したモデルの一部では非常に面白く、報酬の設定しだいでは各エージェントが協力したり敵対したりします。　　

この敵対的に学習を行うというのがGANだと考えているのですが、解析的なアプローチはアカデミアのみなさんが頑張っていることかと思います  

Kerasで敵対的な簡単なマルチエージェントラーニングを21言っちゃダメゲームで構築しました  

21言っちゃダメゲームを行います  
（これはQiitaのはむこさんの記事を参考にさせていただきました、ありがとうございます[3]）

## 強化学習の理論
強化学習は、人間が特に正しい悪いなどを指定せずとも、なんらかの報酬系から報酬を得ることで報酬を最大化します  
このとき、ある系列の状態をSとし、その時の行動をaとし、この組み合わせで得られる報酬関数をQとすると、報酬はのようになります。  
<p align="center">
  <img width="200px" src="https://user-images.githubusercontent.com/4949982/28245373-e7ef3be6-6a3f-11e7-8440-7307f7814321.png">
</p>

また報酬の割引率というものがあるらしく、行動が連続する系では、最終的に得られる得点が各選択に対して何かしら値が振られるのですが、通常どの程度影響を及ぼしているのかわかりません  

（わからないので、今回は具体的に求めるということをしていません）  

今回の例では、Q関数を具体的にDeepLearningによる関数としています  

### ϵ-greedy
全く意識していなかったのですが、どうやら行動の選択は初期値依存性がある程度あり、運が悪いと局所解に嵌ったなままなかなか更新してくれなくなります　　

適度にランダムに行動を選択することを入れないとダメっぽいです　　

## ルール（問題設定）
先手、後手に別れて0から最小１、最大の３つ増やした数字を言い合います　　

数字を累積していって、21以上のになるように言った時点で負けです。相手に21以上を踏ませれば勝ちです  

## 報酬の設定
一個一個の行動に報酬を設定するのは、困難なので、一連の行動の系の結果として勝ったか、負けたかを見ていきます　　

多分、以下の式が本当に最小化すべきことであるのです
<p align="center">
  <img width="300px" src="https://user-images.githubusercontent.com/4949982/28245532-fb6944b0-6a43-11e7-9ec5-76e267fe3a41.png">
</p>

これをもう少し雑に変更します（ここの関数について、最適な選択があり、次に影響しないという仮説を立てると、この式が成り立ちます）　　
<p align="center">
  <img width="420px" src="https://user-images.githubusercontent.com/4949982/28256325-2fd83392-6afc-11e7-9da2-ef1a35e59ce4.png">
</p>
ここでは、具体的なQ関数が具体的な最大の値（１）をもつので、これに直接当てに行きます
<p align="center">
  <img width="480px" src="https://user-images.githubusercontent.com/4949982/28256526-4797306c-6afe-11e7-8ef9-e46865ffb0fd.png">
</p>

勝った時の報酬を+1, 負けた時の報酬を-1のReward関数とすると、以下の式を最小化すれば、勝った時はその選択をより強化して学習し、負けた時は選択を誤ったとして、別の可能性を探索する可能性が強くなります　 

## Q関数の変形
Q関数は状態と次にする行動を入力することで、値を得ますが、今回は状態を入力することで、次の行動を直接もとめます  

具体的には、今の状態から、次の行動を予想します(次の行動を引数に報酬を出力せず、行動を出力)　　

予想した次の行動から、得られた値でもっとも値が大きかった値を次の行動としています  
(この出力をゲーム中保存しておき、一連のゲームの行動とします)  
<p align="center">
  <img width="300px" src="https://user-images.githubusercontent.com/4949982/28245835-280dba4a-6a4a-11e7-8d23-de3bd52b64a3.png">
</p>

## 報酬による学習
ゲームに勝つと、一連の選択が正しかったとして、学習により、より強化されます　　

ゲームに負けると、一連の選択は誤りだったとして、一連の行動を再度選択しないように、学習を行います（こうすることで、別の選択肢が強くなる）

## マルチエージェント
同じような報酬系をもつモデルを２つ以上用意して、対決させました  

先に21以上を言った方が負けというルールで二つのモデルに対決させました  


## コード＆実行
[github](https://github.com/GINK03/keras-multi-agent-tiny-game)にて管理しています　　

マルチエージェント学習
```console
$ python3 21-icchadame.py --reinforce
```

実際に対戦してみる  
```console
$ python3 21-icchadame.py --play
```

## 強さについて
この21言っちゃダメゲームは4の倍数を取りに行けば勝てることがわかっている問題なのですが、途中から4の倍数にはめ込もうとしようとしていることがわかります  

例えば、次のような結果になります  

なお、最適解は、初手で１を打って次に4を取らせることですが、初期値依存性があり、この状態に収束させるのは結構難しいです

例1. 
```console
コンピュータは3を選択しました
now position 3
数字（１−３）を入力してください
2
コンピュータは3を選択しました
now position 8
数字（１−３）を入力してください
1
コンピュータは3を選択しました
now position 12
数字（１−３）を入力してください
2
コンピュータは2を選択しました
now position 16
数字（１−３）を入力してください
1
コンピュータは3を選択しました
now position 20
数字（１−３）を入力してください
1
結果 あなたの負け
```

例2. 
```console
コンピュータは3を選択しました
now position 3
数字（１−３）を入力してください
3
コンピュータは2を選択しました
now position 8
数字（１−３）を入力してください
3
コンピュータは1を選択しました
now position 12
数字（１−３）を入力してください
3
コンピュータは1を選択しました
now position 16
数字（１−３）を入力してください
3
コンピュータは1を選択しました
now position 20
数字（１−３）を入力してください
3
結果 あなたの負け
```
## 参考文献
[1] [Understanding Agent Cooperation](https://deepmind.com/blog/understanding-agent-cooperation/)  
[2] [Multi-agent Reinforcement Learning in Sequential Social Dilemmas](https://storage.googleapis.com/deepmind-media/papers/multi-agent-rl-in-ssd.pdf)  
[3] [深層強化学習：「20言っちゃダメゲーム」の最適解を30分程度で自動的に編み出す（chainerRL）](http://qiita.com/hamko/items/119750780dc430760d78#_reference-4664ea066f5790a8570e)  
